# Requires the latest version of Docker Compose.
# Docker Compose V2 is recommended.
# `docker-compose.yaml` files cannot use shell outputs as their inputs.
# See https://docs.docker.com/compose/compose-file/compose-file-v3
# and https://github.com/compose-spec/compose-spec/blob/master/spec.md
# for details concerning the `docker-compose.yaml` file syntax.
# Variables are in ${VARIABLE:-DEFAULT_VALUE} format
# to ensure that default values are given to the Dockerfile.
# Using a `.env` file to set variables is strongly recommended.
# Run `make env` to create a basic `.env` file with the UID and GID variables.
# Compute Capability must be specified manually via the `CCA` variable.

# Using a `docker-compose.yaml` file has many advantages
# over creating custom shell scripts for each project.
# The settings are much easier to see and maintain than scattered shell scripts.
# Also, Compose is a native Docker component, simplifying project maintenance.

# Set the host environment variable `BUILDKIT_PROGRESS=plain` to see the full build log.
# https://github.com/docker/cli/blob/master/docs/reference/commandline/cli.md#environment-variables


services:
  train:  # Service name. Change the name as necessary for each project.
    hostname: train  # Set to be the same as the service name. Makes terminals easier to tell apart.
    # Remember to use different image names for different users and projects.
    # Otherwise, images will be repeatedly removed and recreated.
    # The removed images will remain cached, however.
    # If no image with the specified image name exists,
    # a new image with that name will be created.
    image: pytorch_source:${TRAIN_NAME:-train}
    # `ipc: host` is a known security vulnerability but removes the shared memory cap.
#    ipc: host  # Equivalent to `--ipc=host` in `docker run`. Disable this for WSL.
#    shm_size: 1GB  # Explicit shared memory limit. No security issues this way.
    tty: true  # Equivalent to `-t` flag in `docker run`.
    init: true  # Equivalent to `--init` flag in `docker run`.
    stdin_open: true  # equivalent to `-i` flag in `docker run`.
    # Docker volumes are the preferred method for connecting to the host filesystem.
    # Setting `HOST_PATH:CONTAINER_PATH` allows the container to access `HOST_PATH` as `CONTAINER_PATH`.
    # See https://docs.docker.com/storage/volumes for details.
    # Current working directory `.` is connected to `PROJECT_ROOT`.
    # PyCharm cannot recognize the $PWD variable for unknown reasons.
    volumes:  # Add volumes as necessary. Equivalent to `-v` flag in `docker run`.
      - .:${PROJECT_ROOT:-/opt/project}  # Use this if the docker-compose.yaml file is at the project root.
#      - ..:${PROJECT_ROOT:-/opt/project}  # Use this if the docker-compose.yaml file is in a subdirectory.
#      - ~/.ssh:/home/user/.ssh  # Bind host SSH configurations to the container for SSH port local forwarding.
#      - ${DATA_PATH_1}:/mnt/data1  # Configurable data path settings for different host data locations.
    build:  # Options for building. Used when `--build` is called in `docker compose`.
      target: train  # Specify build target.
      context: .  # `.dockerignore` should remove all context, making this equivalent to the `Makefile` results.
      dockerfile: Dockerfile
#      cache_from:  # Useful if cache images have been created with the Makefile commands beforehand.
#        - pytorch_source:${INSTALL_NAME:-build_install}
#        - pytorch_source:${TORCH_NAME:-build_torch-${PYTORCH_VERSION_TAG:-v1.10.2}}
      # All arguments given during the build with must be respecified
      # in `args` to prevent a cache miss from occurring.
      # Default values of the `Dockerfile` (but not the `Makefile`) may be omitted.
      args:  # Equivalent to `--build-arg`.
        TORCH_CUDA_ARCH_LIST: ${CCA:?CCA (Compute Capability) is undefined. Check if the .env file exists.}
        PYTORCH_VERSION_TAG: ${PYTORCH_VERSION_TAG:-v1.10.2}
        TORCHVISION_VERSION_TAG: ${TORCHVISION_VERSION_TAG:-v0.11.3}
        TORCHTEXT_VERSION_TAG: ${TORCHTEXT_VERSION_TAG:-v0.11.2}
        TORCHAUDIO_VERSION_TAG: ${TORCHAUDIO_VERSION_TAG:-v0.10.2}
        PROJECT_ROOT: ${PROJECT_ROOT:-/opt/project}
        GID: ${GID:-1000}  # Run `id -g` on the terminal to find your GID. Check that it is set properly in `.env`.
        UID: ${UID:-1000}  # Run `id -u` on the terminal to find your UID. Check that it is set properly in `.env`.
        TZ: ${TZ:-Asia/Seoul}  # Used during the build.
    working_dir: ${PROJECT_ROOT:-/opt/project}
#    ports:  # Uncomment if ports are necessary.
#      - ${PORT:-8080}:22
    user: ${UID:-1000}:${GID:-1000}
    environment:  # Environment variables for the container, not the build. Equivalent to `--env`
      TZ: ${TZ:-Asia/Seoul}  # Used during runtime.
      CUDA_DEVICE_ORDER: PCI_BUS_ID
    deploy:  # API dependent on compose version.
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [ gpu ]
#              device_ids: [ "0" ]  # Use only GPU 0.


  # Use a separate `full.env` file for different configurations if necessary.
  full:  # Service for `*-full` installs.
    hostname: full
    image: pytorch_source:${TRAIN_NAME_FULL:-full}
#    ipc: host
#    shm_size: 1GB
    tty: true
    init: true
    stdin_open: true
    volumes:
      - .:${PROJECT_ROOT:-/opt/project}
#      - ..:${PROJECT_ROOT:-/opt/project}  # Use this if docker-compose.yaml is in a subdirectory of another project.
#      - ~/.ssh:/home/user/.ssh  # Bind host SSH configurations to the container for SSH port local forwarding.
#      - ${DATA_PATH_1}:/mnt/data1  # Configurable data path settings for different host data locations.
    build:
      # Set `TARGET_STAGE_FULL` to `train-builds` to get just the wheels in `/tmp/dist`.
      target: ${TARGET_STAGE_FULL:-train}
      context: .
      dockerfile: Dockerfile
#      cache_from:
#        # Using defaults inside defaults, which may be hard to read.
#        - pytorch_source:${INSTALL_NAME_FULL:-build_install-${LINUX_DISTRO:-ubuntu}${DISTRO_VERSION:-18.04}-cuda${CUDA_VERSION:-10.2}-cudnn${CUDNN_VERSION:-8}-py${PYTHON_VERSION:-3.9}}
#        - pytorch_source:${TORCH_NAME_FULL:-build_torch-${PYTORCH_VERSION_TAG:-v1.10.2}-${LINUX_DISTRO:-ubuntu}${DISTRO_VERSION:-18.04}-cuda${CUDA_VERSION:-10.2}-cudnn${CUDNN_VERSION:-8}-py${PYTHON_VERSION:-3.9}}
      args:  # Equivalent to `--build-arg`. Set to default values for `*-full`.
        BUILD_CAFFE2: 0  # Caffe2 disabled for faster build.
        BUILD_CAFFE2_OPS: 0
        BUILD_TEST: 0
        USE_NNPACK: 0
        USE_QNNPACK: 0
        DEBUG: ${DEBUG:-0}
#        CLEAN_CACHE_BEFORE_BUILD: ${CLEAN_CACHE_BEFORE_BUILD:-0}
        LINUX_DISTRO: ${LINUX_DISTRO:-ubuntu}
        DISTRO_VERSION: ${DISTRO_VERSION:-18.04}
        CUDA_VERSION: ${CUDA_VERSION:-10.2}
        CUDNN_VERSION: ${CUDNN_VERSION:-8}
        PYTHON_VERSION: ${PYTHON_VERSION:-3.9}
        MAGMA_VERSION: ${MAGMA_VERSION:-102}
        MKL_MODE: ${MKL_MODE:-include}  # MKL_MODE can be `include` or `exclude`.
        TORCH_CUDA_ARCH_LIST: ${CCA:?CCA (Compute Capability) is undefined. Check if the .env file exists.}
        PYTORCH_VERSION_TAG: ${PYTORCH_VERSION_TAG:-v1.10.2}
        TORCHVISION_VERSION_TAG: ${TORCHVISION_VERSION_TAG:-v0.11.3}
        TORCHTEXT_VERSION_TAG: ${TORCHTEXT_VERSION_TAG:-v0.11.2}
        TORCHAUDIO_VERSION_TAG: ${TORCHAUDIO_VERSION_TAG:-v0.10.2}
        PROJECT_ROOT: ${PROJECT_ROOT:-/opt/project}
        GID: ${GID:-1000}
        UID: ${UID:-1000}
        TZ: ${TZ:-Asia/Seoul}
    working_dir: ${PROJECT_ROOT:-/opt/project}
    user: ${UID:-1000}:${GID:-1000}
#    depends_on:
#      - pull
    environment:
      TZ: ${TZ:-Asia/Seoul}
      CUDA_DEVICE_ORDER: PCI_BUS_ID
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [ gpu ]


  deploy:
    hostname: deploy
    image: pytorch_source:${DEPLOY_NAME:-deploy}
    tty: true
    init: true
    stdin_open: true
    volumes:
      - .:${PROJECT_ROOT:-/opt/project}
    build:
      target: deploy
      context: .
      dockerfile: Dockerfile
      args:
        # The Anaconda `defaults` channel is not free for commercial use.
        BUILD_TEST: 1  # Enable build tests for deployment.
        BUILD_CAFFE2: 1  # Caffe2 should be enabled in production settings.
        BUILD_CAFFE2_OPS: 1
        USE_NNPACK: 1  # Enable NNPack for deployment.
        USE_QNNPACK: 1  # Enable QNNPack for deployment.
        CLEAN_CACHE_BEFORE_BUILD: 1  # Cache should not affect reproducibility.
        LINUX_DISTRO: ${LINUX_DISTRO:-ubuntu}
        DISTRO_VERSION: ${DISTRO_VERSION:-20.04}
        CUDA_VERSION: ${CUDA_VERSION:-11.3.1}
        CUDNN_VERSION: ${CUDNN_VERSION:-8}
        PYTHON_VERSION: ${PYTHON_VERSION:-3.9}
        MAGMA_VERSION: ${MAGMA_VERSION:-113}
        # Requirements must include `mkl` if `MKL_MODE` is set to `include` for deployment.
        MKL_MODE: ${MKL_MODE:-exclude}  # `include` or `exclude`. Disabled by default for deployment.
        TORCH_CUDA_ARCH_LIST: ${CCA:?CCA (Compute Capability) is undefined. Check if the .env file exists.}
        PYTORCH_VERSION_TAG: ${PYTORCH_VERSION_TAG:-v1.10.2}
        TORCHVISION_VERSION_TAG: ${TORCHVISION_VERSION_TAG:-v0.11.3}
        TORCHTEXT_VERSION_TAG: ${TORCHTEXT_VERSION_TAG:-v0.11.2}
        TORCHAUDIO_VERSION_TAG: ${TORCHAUDIO_VERSION_TAG:-v0.10.2}
        PROJECT_ROOT: ${PROJECT_ROOT:-/opt/project}
    working_dir: ${PROJECT_ROOT:-/opt/project}
    environment:
      CUDA_DEVICE_ORDER: PCI_BUS_ID
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [ gpu ]

  # This layer may be useful for PyTorch contributors.
  # It is also the base for developing new layers for new builds.
  devel:  # Skeleton service for development and debugging.
    hostname: devel
    image: pytorch_source:${DEVEL_NAME:-devel}
    tty: true
    init: true
    stdin_open: true
    volumes:
      - .:${PROJECT_ROOT:-/opt/project}
    build:
      target: ${TARGET_STAGE:-build-base}  # All builds begin at `build-base`.
      context: .
      dockerfile: Dockerfile
      args:
        TORCH_CUDA_ARCH_LIST: ${CCA:?CCA (Compute Capability) is undefined. Check if the .env file exists.}


  ngc:  # NGC image service.
    hostname: ngc
    image: pytorch_source:ngc-${YEAR:-22}.${MONTH:-01}
#    ipc: host
#    shm_size: 1GB
    tty: true
    init: true
    stdin_open: true
    volumes:
      - .:${PROJECT_ROOT:-/opt/project}
    build:
      target: ngc
      context: .
      dockerfile: ngc.Dockerfile
      args:
        PROJECT_ROOT: ${PROJECT_ROOT:-/opt/project}
        YEAR: ${YEAR:-22}
        MONTH: ${MONTH:-01}
        GID: ${GID:-1000}
        UID: ${UID:-1000}
        TZ: ${TZ:-Asia/Seoul}
    working_dir: ${PROJECT_ROOT:-/opt/project}
    user: ${UID:-1000}:${GID:-1000}
    environment:
      TZ: ${TZ:-Asia/Seoul}
      CUDA_DEVICE_ORDER: PCI_BUS_ID
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [ gpu ]


  pull:  # Simple utility to explicitly pull and save the base Docker image.
    image: nvidia/cuda:${CUDA_VERSION}-cudnn${CUDNN_VERSION}-devel-${LINUX_DISTRO}${DISTRO_VERSION}


# https://docs.docker.com/storage/volumes/#use-a-volume-with-docker-compose
# https://docs.docker.com/compose/compose-file/compose-file-v3/#volumes
# https://github.com/compose-spec/compose-spec/blob/master/spec.md#volumes-top-level-element
volumes:
  shared:  # Maybe use later to share data between different containers.
  # See # https://docs.docker.com/storage/volumes/#share-data-among-machines
