# 범용 PyTorch 소스 빌드 Docker 템플릿

## Preamble
최근 몇 년 동안 더 작고 효율적인 장치에서 계속 증가하는 데이터 양에 대처하기 위해
효율적인 신경망을 설계하고 구현하는 데 엄청난 학문적 노력이 투입되었습니다.
그러나 이 글을 쓰는 시점에서 대부분의 딥 러닝 실무자들은 가장 기본적인 GPU 가속 기술조차 모르고 있습니다.

특히 학계에서는 메모리 요구 사항을 1/4로 줄이고,
속도를 4~5배 높일 수 있는 AMP(Automatic Mixed Precision)조차 사용하지 않는 경우가 많습니다.
[HuggingFace Accelerate](https://github.com/huggingface/accelerate) 또는 [PyTorch Lightning](https://github.com/PyTorchLightning/pytorch-lightning)를 사용하여 큰 번거로움 없이 AMP를 활성화할 수 있음에도 마찬가지입니다.
특히 Accelerate 라이브러리는 몇 줄의 코드만으로 기존 PyTorch 프로젝트에 통합할 수 있습니다.

딥 러닝의 신비에 발을 담그기 시작한 초보자라도 더 많은 컴퓨팅이 성공의 핵심 요소라는 것을 알고 있습니다.
과학자가 아무리 똑똑하더라도 10배 더 많은 컴퓨팅으로 경쟁자를 능가하는 것은 결코 대단한 일이 아닙니다.

이 템플릿은 GPU, CUDA, Docker 등에 대한 지식이 많지 않은 연구원과 엔지니어가 __*동일한 하드웨어와 신경망을 사용하여*__ GPU의 성능을 최대한 끌어낼 수 있도록 하기 위해 만들어졌습니다.

PyTorch 소스 빌드가 포함된 Docker 이미지는 이미 공식 [PyTorch Docker Hub](https://hub.docker.com/r/pytorch/pytorch)리포지토리와 [NVIDIA NGC](https://ngc.nvidia.com/catalog/containers/nvidia:pytorch) 리포지토리에서 사용할 수 있지만 이러한 이미지에는 다른 패키지가 많이 설치되어 있어 기존 프로젝트에 통합하기 어렵습니다.
또한 많은 실무자는 Docker 이미지보다 로컬 환경을 사용하는 것을 선호합니다.

여기에 제시된 프로젝트는 다릅니다.
사용자가 설치한 라이브러리를 제외하고 작업할 추가 라이브러리가 없습니다.
빌드에서 생성된 휠은 Docker 사용법을 배울 필요 없이 모든 환경에서 사용하기 위해 추출할 수 있습니다.
 (이 프로젝트의 두 번째 부분에서는 Docker를 훨씬 쉽게 사용할 수 있도록 `docker-compose.yaml` 파일도 제공합니다.)

만약 당신이 더 빨리 끝내고 싶어하는 사람이라면 Tensor board를 응시하면서 오랜 시간을 견디는것이 끝났습니다.
이 프로젝트가 바로 정답일 수 있습니다.
AMP와 결합된 최신 버전의 CUDA와 함께 PyTorch의 소스 빌드를 사용할 때,
순수한 PyTorch 환경보다 학습/추론시간을 10배 빠르게 달성할 수 있습니다.

제 프로젝트가 학계와 산업계의 실무자들에게 도움이 되기를 진심으로 바랍니다.
제 작업이 유익하다가 생각하시는 사용자는 이 저장소에 star을 해주셔서 감사를 표시해주시는 것을 환영합니다.


## Warning
__*이 템플릿을 사용하기 전에 먼저 GPU를 실제로 사용하고 있는지 확인하세요!*__

대부분의 시나리오에서, 느린 학습은 비효율적인 파이프라인 ETL(추출, 변환, 로드)에 의해 발생합니다.
데이터가 GPU가 느리게 실행되기 때문이 아니라 데이터가 GPU에  충분히 빠르게 도달하지 않아서 학습이 느립니다.
GPU 사용률이 컴퓨팅 최적화를 할 만큼 충분히 높은지 확인하려면 `watch nvidia-smi`를 실행하세요.
GPU 사용률이 낮거나 돌발적으로 최고조에 달하는 경우, 이 템플릿을 사용하기 전에 효율적인 ETL 파이프라인을 설계하세요.
그렇지 않으면, 더 빠른 컴퓨팅이 병목 현상이 되지 않으므로 별로 도움이 되지 않습니다.

효율적인 ETL 파이프라인 설계에 대한 가이드는 https://www.tensorflow.org/guide/data_performance 를 참조하세요.

[NVIDIA DALI](https://github.com/NVIDIA/DALI) 라이브러리 또한 도움이 될 수도 있습니다. 
The [DALI PyTorch plugin](https://docs.nvidia.com/deeplearning/dali/user-guide/docs/plugins/pytorch_tutorials.html)
은 PyTorch에서 효율적인 ETL 파이프라인을 위한 API를 제공합니다.


## Introduction
PyTorch/CUDA/cuDNN의 __*모든 버전*__ 에서의 __*소스로부터*__ PyTorch를 빌드하기 위한 템플릿 리포지토리.

소스에서 빌드한 PyTorch는 `pip`/`conda`에서 설치된 PyTorch보다 훨씬 빠르지만(일부 벤치마크에서는 x4배, x2가 더 일반적입니다.
소스에서 빌드하는 것은 힘들고 버그가 발생하기 쉬운 프로세스입니다.

이 리포지토리는 모든 버전의 CUDA에서 의 소스로부터 모든 버전의 PyTorch를 빌드하기 위한 고도로 모듈화된 템플릿입니다.
ILinux 기반 이미지 또는 프로젝트에 통합할 수 있는 사용하기 쉬운 Dockerfile을 제공합니다.

For researchers unfamiliar with Docker, 
the generated wheel files can be extracted 
to install PyTorch on their local environments.

Windows users may also use this project via WSL. See instructions below.

A `Makefile` is provided both as an interface for easy use and as 
a tutorial for building custom images.

A `docker-compose.yaml` file is also provided for a simple interactive development experience using Docker.

The speed gains from this template come from the following factors:
1. Using the latest version of CUDA and associated libraries (cuDNN, cuBLAS, etc.).
2. Using a source build made specifically for the target machine with the latest software customizations
instead of a build that must be compatible with different hardware and software environments.
3. Using the latest version of PyTorch and subsidiary libraries. 
Many users do not update their PyTorch
version because of compatibility issues with their pre-existing environment.
4. Informing users on where to look for solutions to their speed problems 
(this may be the most important factor).

Combined with techniques such as AMP and cuDNN benchmarking, 
computational throughput can be increased dramatically 
(e.g., x10) __*on the same hardware*__.

Even if you do not wish to use Docker in your project,
you may still find this template useful.

**_The wheel files generated by the build can be used in any Python environment with no dependency on Docker._**

This project can thus be used to generate custom wheel files, 
improving both training and inference speeds dramatically, 
for any desired environment (`conda`, `pip`, etc.).


## Quickstart
__*Users are free to customize the `train` stage of the `Dockerfile` as they please. 
However, do not change the `build` stages unless absolutely necessary.
If a new package must be built, add a new `build` layer.*__

This project is a template, and users are expected to customize it to fit their needs.

The code is assumed to be running on a Linux host with 
the necessary NVIDIA Drivers and a recent version of Docker & Docker Compose pre-installed.
If this is not the case, install these first.

To build a training image, first edit the Dockerfile `train` stage to include 
desired packages from `apt`/`conda`/`pip`. 

Then, visit https://developer.nvidia.com/cuda-gpus to find the
Compute Capability (CC) of the target GPU device.

Finally, run `make all CC=TARGET_CC(s)`.


### Examples 
(1) `make all CC="8.6"` for RTX 3090, 
(2) `make all CC="7.5 8.6"` for both RTX 2080Ti and RTX 3090 
(building for many GPU CCs will increase build time).

This will result in an image, `pytorch_source:train`, which can be used for training.

Note that CCs for devices not available during the build can be used to build the image.
For example, if the image must be used on an RTX 2080Ti machine but the user only has an RTX 3090, 
the user can set `CC="7.5"` to enable the image to operate on the RTX 2080Ti GPU.
See https://pytorch.org/docs/stable/cpp_extension.html 
for an in-depth guide on how to set `TORCH_CUDA_ARCH_LIST`, 
which is specified by `CC` in the `Makefile`.


### Makefile Explanation
The `Makefile` is designed to make using this package simple and modular.

The first image to be created is `pytorch_source:build_install`, 
which contains all packages necessary for the build.
The installation image is created separately to cache downloads.

The second image is `pytorch_source:build_torch-v1.9.1` (by default), 
which contains the wheels for PyTorch, TorchVision, TorchText, and TorchAudio
with settings for PyTorch 1.9.1 on Ubuntu 20.04 LTS with Python 3.8, CUDA 11.3.1 and cuDNN 8.
The second image exists to cache the results of the build process.

If you do not wish to use Docker and would like to only extract 
the `.whl` wheel files for a pip install on your environment,
the generated wheel files can be found in the `/tmp/dist` directory.

Saving the build results also allows for more convenient version switching in case
different PyTorch versions (different CUDA version, different library version, etc.) are needed.

The final image is `pytorch_source:train`, which is the image to be used for actual training.
It relies on the previous stages only for the build artifacts (wheels, etc.) and nothing else.
This makes it very simple to create different training images optimized for different environments and GPU devices.

Because PyTorch has already been built, 
the training image only needs to download the 
remaining `apt`/`conda`/`pip` packages. 
Caching is also implemented to speed up even this process.


### Timezone Settings
International users may find this section helpful.

The `train` image has its timezone set by the 
`TZ` variable using the `tzdata` package.
The default timezone is `Asia/Seoul` but this can be changed by 
specifying the `TZ` variable when calling `make`.
Use [IANA](https://www.iana.org/time-zones) 
timezone names to specify the desired timezone.

Example: `make all CC="8.6" TZ=America/Los_Angeles` uses L.A. time on the training image.

NOTE: Only the training image has timezone settings. 
The installation and build images do not use timezone information.

In addition, the training image has `apt` and `pip` 
installation URLs updated for Korean users.
If you wish to speed up your installs, 
please find URLs optimized for your location, 
though the installation caches may make this unnecessary.


## Specific PyTorch Version
__*PyTorch subsidiary libraries only work with matching versions of PyTorch.*__

To change the version of PyTorch,
set the [`PYTORCH_VERSION_TAG`](https://github.com/pytorch/pytorch), 
[`TORCHVISION_VERSION_TAG`](https://github.com/pytorch/vision), 
[`TORCHTEXT_VERSION_TAG`](https://github.com/pytorch/text), and 
[`TORCHAUDIO_VERSION_TAG`](https://github.com/pytorch/audio) 
variables to matching versions.

The `*_TAG` variables must be GitHub tags or branch names of those repositories.
Visit the GitHub repositories of each library to find the appropriate tags.

Example: To build on an RTX 3090 GPU with PyTorch 1.9.1, use the following command:

`make all CC="8.6" 
PYTORCH_VERSION_TAG=v1.9.1 
TORCHVISION_VERSION_TAG=v0.10.1 
TORCHTEXT_VERSION_TAG=v0.10.1
TORCHAUDIO_VERSION_TAG=v0.9.1`.

The resulting image, `pytorch_source:train`, can be used 
for training with PyTorch 1.9.1 on GPUs with Compute Capability 8.6.


## Multiple Training Images
To use multiple training images on the same host, 
give a different name to `TRAIN_NAME`, 
which has a default value of `train`.

New training images can be created without having to rebuild PyTorch
if the same build image is used for different training images.
Creating new training images takes only a few minutes at most.

This is useful for the following use cases.
1. Allowing different users, who have different UID/GIDs, 
to use separate training images.
2. Using different versions of the final training image with 
different library installations and configurations.
3. Using this template for multiple PyTorch projects,
each with different libraries and settings.

For example, if `pytorch_source:build_torch-v1.9.1` has already been built,
Alice and Bob would use the following commands to create separate images.

Alice:
`make build-train 
CC="8.6"
TORCH_NAME=build_torch-v1.9.1
PYTORCH_VERSION_TAG=v1.9.1
TORCHVISION_VERSION_TAG=v0.10.1
TORCHTEXT_VERSION_TAG=v0.10.1
TORCHAUDIO_VERSION_TAG=v0.9.1
TRAIN_NAME=train_alice`

Bob:
`make build-train 
CC="8.6"
TORCH_NAME=build_torch-v1.9.1
PYTORCH_VERSION_TAG=v1.9.1
TORCHVISION_VERSION_TAG=v0.10.1
TORCHTEXT_VERSION_TAG=v0.10.1
TORCHAUDIO_VERSION_TAG=v0.9.1
TRAIN_NAME=train_bob` 

This way, Alice's image would have her UID/GID while Bob's image would have his UID/GID.
This procedure is necessary because training images have their users set during the build.
Also, different users may install different libraries in their training images.
Their environment variables and other settings may also be different.


### Word of Caution
When using build images such as `pytorch_source:build_torch-v1.9.1` as a build cache 
for creating new training images, the user must re-specify all build arguments 
(variables specified by ARG and ENV using --build-arg) of all previous layers.

Otherwise, the default values for these arguments will be given to the Dockerfile
and a cache miss will occur because of the different input values.

This will both waste time rebuilding previous layers and, more importantly,
cause inconsistency in the training images due to environment mismatch.

This includes the `docker-compose.yaml` file as well. 
All arguments given to the `Dockerfile` during the build must be respecified.
This includes default values present in the `Makefile` 
but not present in the `Dockerfile` such as the version tags.

__*If Docker starts to rebuild layers that you have already built, 
suspect that build arguments have been given incorrectly.*__ 

See https://docs.docker.com/develop/develop-images/dockerfile_best-practices/#leverage-build-cache
for more information.

The `BUILDKIT_INLINE_CACHE` must also be given to an image to use it as a cache later. See 
https://docs.docker.com/engine/reference/commandline/build/#specifying-external-cache-sources
for more information.


## Advanced Usage
The `Makefile` provides the `*-full` commands for advanced usage.

`make all-full CC=YOUR_GPU_CC TRAIN_NAME=train_cu102` will create 
`pytorch_source:build_install-ubuntu18.04-cuda10.2-cudnn8-py3.9`,
`pytorch_source:build_torch-v1.9.1-ubuntu18.04-cuda10.2-cudnn8-py3.9`, 
and `pytorch_source:train_cu102` by default.

These images can be used for training/deployment on CUDA 10 devices such as the GTX 1080Ti.

Also, the `*-clean` commands are provided to check for cache reliance on previous builds.


### Specific CUDA Version
Set `CUDA_VERSION`, `CUDNN_VERSION`, and `MAGMA_VERSION` to change CUDA versions.
`PYTHON_VERSION` may also be changed if necessary.

This will create a build image that can be used as a cache 
to create training images with the `build-train` command.

Also, the extensive use of caching in the project means that 
the second build is much faster than the first build.
This may be advantageous if many images must be created for multiple PyTorch/CUDA versions.

### Specific Linux Distro
CentOS and UBI images can be created with only minor edits to the `Dockerfile`.
Read the `Dockerfile` for full instructions.

Set the `LINUX_DISTRO` and `DISTRO_VERSION` arguments afterwards.

### Windows
Windows users may use this template by updating to Windows 11 and installing 
Windows Subsystem for Linux (WSL).
WSL on Windows 11 gives a similar experience to using native Linux.

This project has been tested on WSL on Windows 11 
with the WSL CUDA driver and Docker Desktop for Windows.


# Interactive Development with Docker Compose

## _Raison D'être_
The purpose of this section is to introduce 
a new paradigm for deep learning development. 
I hope that using Docker Compose for deep learning projects
will eventually become best practice, 
improving the reproducibility of ML experiments 
and freeing ordinary researchers from the burden 
of managing their development environments.

Developing in local environments with `conda` or `pip` 
is commonplace in the deep learning community.
However, this risks making the development environment, 
and the code meant to run on it, unreproducible.
This is a serious detriment to scientific progress 
that many readers of this article 
will have experienced at first-hand.

Docker containers are the standard method for
providing reproducible programs 
across different computing environments. 
They create isolated environments where programs 
can run without interference from the host or from one another.
See https://www.docker.com/resources/what-container for details.

But in practice, Docker containers are often misused. 
Containers are meant to be transient and best practice dictates that 
a new container be created for each run.
But this is very inconvenient for development, 
especially for deep learning applications, 
where new libraries must constantly be installed and 
bugs are often only evident at runtime.
This leads many researchers to develop inside interactive containers.
Docker users often have `run.sh` files with commands such as
`docker run -v my_data:/mnt/data -p 8080:22 -t my_container my_image:latest /bin/bash`
(does this look familiar to anyone?) and use SSH to connect to running containers.
VSCode also provides a remote development mode that can be used 
to code inside containers.

The problem with this approach is that these interactive containers 
become just as unreproducible as local development environments.
A running container cannot connect to a new port or attach a new volume.
But if the computing environment within the container was created over several months 
of installs and builds, the only way to keep it 
is to save it as an image and create a new container from the saved image.
After a few iterations of this process, 
the resulting image becomes bloated and completely unreproducible.

To alleviate this problem, a `docker-compose.yaml` 
file is provided for easy management of containers.
Docker Compose is part of Docker and is already 
a popular tool for both development and production.
For unknown reasons, it has not taken off in the deep learning community yet, 
though anyone who knows how to use Docker will find Compose fairly simple.
This may be because Compose is often advertised as a multi-container solution,
though it can also be used for single-container development just as well.

The `docker-compose.yaml` file allows the user to specify settings 
for both build and run.
Connecting a new volume is as simple as removing the current container,
adding a line in the `docker-compose.yaml`/`Dockerfile` file, 
then creating a new container from the same image. 
Build caches allow new images to be built very quickly,
removing another barrier to Docker adoption,
the long initial build time.

The instructions below allow interactive development on the terminal,
making the transition from local development to 
Docker and Docker Compose much smoother.

With luck, the deep learning community will be able to 
"_code once, train anywhere_" with this technique.
But even if I fail in persuading the majority of users 
of the merits of my method,
I may still spare many a hapless grad student from the 
sisyphean labor of setting up their `conda` environment,
only to have it crash and burn right before their paper submission is due.


## Usage
__*Docker images created by the `Makefile` 
are fully compatible with the `docker-compose.yaml` file.
There is no need to erase them to use Docker Compose.*__

Using Docker Compose V2 (see https://docs.docker.com/compose/cli-command),
run the following two commands, where `train` is the default service name 
in the provided `docker-compose.yaml` file.

0. Read `docker-compose.yaml` and set variables in the `.env` file (first time only).
1. `docker compose up -d train`
2. `docker compose exec train /bin/bash`

This will open an interactive shell with settings specified by the `train` service 
in the `docker-compose.yaml` file. 
Environment variables can be saved in a `.env` file placed on the project root,
removing the need to type in variables such as UID/GID values with each run.
To create a basic `.env` file, run `make env`.

This is extremely convenient for managing reproducible development environments.
For example, if a new `pip` or `apt` package must be installed for the project,
users can simply edit the `train` layer of the 
`Dockerfile` by adding the package to the 
`apt-get install` or `pip install` commands, 
then run the following command:

`docker compose up -d --build train`.

This will remove the current `train` session, rebuild the image, 
and start a new `train` session.
It will not, however, rebuild PyTorch (assuming no cache miss occurs).
Users thus need only wait a few minutes for the additional downloads, 
which are accelerated by caching and with fast mirror URLs.

To stop and restart a service after editing the 
`Dockerfile` or `docker-compose.yaml` file,
simply run `docker compose up -d train` again.

To remove all Compose containers, use the following:

`docker compose down`.

Users with remote servers may use Docker contexts
(see https://docs.docker.com/engine/context/working-with-contexts)
to access their containers from their local environments.
For more information on Docker Compose, see the documentation
https://github.com/compose-spec/compose-spec/blob/master/spec.md.


## Compose as Best Practice

I wish to emphasize that using Docker Compose in this manner 
is a general-purpose technique 
that does not depend on anything about this project.
As an example, an image from the NVIDIA NGC PyTorch repository 
has been used as the base image in `ngc.Dockerfile`.
The NVIDIA NGC PyTorch images contain many optimizations 
for the latest GPU architectures and provides
a multitude of pre-installed machine learning libraries. 
For anyone starting a new project, and therefore with no dependencies,
using the latest NGC image is recommended.

To use the NGC images, use the following commands:

1. `docker compose up -d ngc`
2. `docker compose exec ngc /bin/bash`

The only difference with the previous `train` session is the session name.


# Known Issues

1. Connecting to a running container by `ssh` will remove all variables set by `ENV`.
This is because `sshd` starts a new environment, wiping out all previous variables.
Using `docker`/`docker compose` to enter containers is strongly recommended.

2. Building on CUDA 11.4.x is not available as of October 2021 because `magma-cuda114`
has not been released on the `pytorch` channel of anaconda.
Users may attempt building with older versions of `magma-cuda` 
or try the version available on `conda-forge`.
A source build of `magma` would be welcome as a pull request.

3. Ubuntu 16.04 build fails. 
This is because the default `git` installed by `apt` on 
Ubuntu 16.04 does not support the `--jobs` flag. 
Add the `git-core` PPA to `apt` and install the latest version of git.
Also, PyTorch v1.9+ will not build on Ubuntu 16. 
Lower the version tag to v1.8.2 to build.
However, the project will not be modified to accommodate 
Ubuntu 16.04 builds as Xenial Xerus has already reached EOL.


# Desiderata

0. **MORE STARS**. If you are reading this, star this repository immediately. I'm serious.

1. CentOS and UBI images have not been implemented yet.
As they require only simple modifications, 
pull requests implementing them would be very much welcome.

2. Translations into other languages are welcome. 
Please make a separate `LANG.README.md` file and create a PR.

3. Please feel free to share this project! I wish you good luck and happy coding!
